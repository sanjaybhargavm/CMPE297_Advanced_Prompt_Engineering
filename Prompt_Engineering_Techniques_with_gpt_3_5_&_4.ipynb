{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnpA_t6RPa0w"
      },
      "source": [
        "###Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "nX9sSMa6zRHT",
        "outputId": "25b6fe60-8d88-4e44-9796-814f423400d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.3.3\n",
            "    Uninstalling openai-1.3.3:\n",
            "      Successfully uninstalled openai-1.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0PBKj_u6zh5I"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "api_key = \" openai api key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nERftib5yrXj"
      },
      "source": [
        "###**ICL (In-Context Learning)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Omoq-R1RQW"
      },
      "source": [
        "ICL: Success Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmPW6f310F0P",
        "outputId": "816ca9bf-43c9-48db-efbf-b513eaaae5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To make a basic cup of coffee, you will need the following:\n",
            "\n",
            "Ingredients:\n",
            "- Coffee beans (or pre-ground coffee)\n",
            "- Water\n",
            "\n",
            "Equipment:\n",
            "- Coffee grinder (if using whole beans)\n",
            "- Coffee maker (such as a drip coffee maker)\n",
            "- Coffee filter\n",
            "- Coffee mug\n",
            "- Measuring tools (such as a coffee scoop or kitchen scale)\n",
            "\n",
            "Here's a step-by-step guide on making coffee:\n",
            "\n",
            "1. Measure the water: Start by measuring the amount of water you'll need. A standard ratio is about 1 tablespoon (or 7 grams) of coffee for every 6 ounces (180 milliliters) of water. Adjust the amount based on your preferences and the strength of coffee you desire.\n",
            "\n",
            "2. Grind the coffee beans (if using whole beans): If you have whole coffee beans, grind them to a medium-fine consistency using a coffee grinder. Different coffee makers may require different grind sizes, so check the instructions for your specific coffee maker.\n",
            "\n",
            "3. Set up the coffee maker: Follow the instructions for your coffee maker to set it up properly. This typically involves adding water to the reservoir and placing a coffee filter in the designated area.\n",
            "\n",
            "4. Add the coffee grounds: Add the appropriate amount of coffee grounds to the coffee filter. For pre-ground coffee, simply spoon it into the filter. If using a coffee scoop, use one scoop per serving. If using a kitchen scale, measure the desired amount based on the brewing ratio mentioned earlier.\n",
            "\n",
            "5. Brew the coffee: Turn on the coffee maker and let it brew. The brewing process will vary depending on the type of coffee maker you have. Be sure to follow the instructions provided by the manufacturer.\n",
            "\n",
            "6. Serve and enjoy: Once the brewing is complete, carefully remove the coffee pot or carafe from the coffee maker. Pour the brewed coffee into a coffee mug or cup. Add any desired milk, cream, or sugar to taste. Stir gently and enjoy your cup of coffee!\n",
            "\n",
            "Remember, this is just a basic method for making coffee. There are many variations and techniques you can explore to customize your coffee to your liking.\n"
          ]
        }
      ],
      "source": [
        "# Define a list of messages for the conversation\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an assistant to a trader.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me how to make a coffee.\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOCo-NCz1JIH"
      },
      "source": [
        "Failure Case for In-context Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qTniujJyUx_",
        "outputId": "978390e5-26bf-4a56-bb67-51a92353cce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but as an AI language model, I don't have access to real-time data. The stock price of Apple may vary throughout the day as it is influenced by market factors such as supply and demand, economic news, and investor sentiment. I recommend checking a financial news website, a stock market app, or contacting your broker for the most up-to-date information on Apple's stock price.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an assistant to a trader.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about the market today.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's the stock price of apple today\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",  # You can specify the GPT-3.5 model or GPT-4 if available\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9HN6wgJ2J-D"
      },
      "source": [
        "###**(CoT) Chain of Thoughts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD_6YXt74Sav"
      },
      "source": [
        "Failure Case: CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTTm-eJc4Rry",
        "outputId": "6dbfc8c4-76a0-4b4a-ffa8-caf6a66da074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To answer that question, I would need to know the distance between the starting point and point B.\n"
          ]
        }
      ],
      "source": [
        "# Define a list of messages for a CoT conversation to solve a math problem\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"there is a train travelling at 50 mph how long will it take to reach point b\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUgPGdDZ4VTp"
      },
      "source": [
        "Success Case: CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWxfUHeA1iVl",
        "outputId": "737599f3-549b-4e1e-8096-9dfd4586f01d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To find the total number of oranges, multiply the number of baskets (4) by the number of oranges in each basket (8). \n",
            "\n",
            "4 baskets x 8 oranges per basket = 32 oranges \n",
            "\n",
            "So, you have a total of 32 oranges.\n"
          ]
        }
      ],
      "source": [
        "# Define a list of messages for a CoT conversation to solve a math problem\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I have 3 boxes, and each box contains 5 apples. How many apples do I have in total?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Let's solve this step by step. You have 3 boxes, and each contains 5 apples.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"To find the total number of apples, multiply the number of boxes (3) by the number of apples in each box (5).\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"3 boxes x 5 apples per box = 15 apples.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"So, you have a total of 15 apples.\"},\n",
        "    {\"role\": \"user\", \"content\": \"You have 4 baskets, and each basket contains 8 oranges. How many oranges do you have in total?\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0WLFknCn2vU"
      },
      "source": [
        "###**Iterative Chain of Thoughts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3o5IUiNn0HD",
        "outputId": "84f383f6-a9dd-4ac6-b64c-82d573038089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have a total of 10 mangoes.\n",
            "User: how many mangoes if you have 5 boxes each having 2 mangoes\n",
            "AI: If you have 2 boxes, and each box contains 5 mangoes, you would have a total of 2 x 5 = 10 mangoes.\n",
            "\n",
            "Similarly, if you have 5 boxes, and each box contains 2 mangoes, you would have a total of 5 x 2 = 10 mangoes.\n",
            "User: end\n",
            "AI: If each box contains 5 mangoes, and you have 2 boxes, then you have a total of 5 mangoes + 5 mangoes = 10 mangoes.\n",
            "User: done\n",
            "Math tutor session ended.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# Define a list of messages for a CoT conversation to solve math problems\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I have 2 boxes, and each box contains 5 mangoes. How many mangoes do I have in total?\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the initial response\n",
        "print(reply)\n",
        "\n",
        "# Continue the conversation iteratively\n",
        "while True:\n",
        "    user_input = input(\"User: \")  # Get user's input\n",
        "    if user_input.lower() == \"done\":\n",
        "        break  # Exit the loop if the user is done\n",
        "    else:\n",
        "        # Append the user's message to the conversation\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Generate a response from the model\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=messages,\n",
        "            api_key=api_key\n",
        "        )\n",
        "\n",
        "        # Get the model's reply from the response\n",
        "        reply = response['choices'][0]['message']['content']\n",
        "\n",
        "        # Print the model's reply\n",
        "        print(\"AI: \" + reply)\n",
        "\n",
        "# End the conversation\n",
        "print(\"Math tutor session ended.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3JH79ri-U5i"
      },
      "source": [
        "###**Tree-of-Thoughts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiGOyy7C-hdB",
        "outputId": "fe77d58a-2e45-4350-ef75-fbc02a282692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcPHffAQ-w5t",
        "outputId": "5f13d953-af24-4627-d69e-24e3f48ba7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.337-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.65-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.337 langsmith-0.0.65 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Fg_aEWvg8Da9"
      },
      "outputs": [],
      "source": [
        "# MVP of an enhanved ToT reasoning technique\n",
        "import dotenv\n",
        "import os\n",
        "from langchain import PromptTemplate, OpenAI, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BEd0X2fw-Z-F"
      },
      "outputs": [],
      "source": [
        "dotenv.load_dotenv('abc.env')\n",
        "openai_api_key = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "dyoyw6KG-6Rm"
      },
      "outputs": [],
      "source": [
        "# Invoke conversation chain\n",
        "chat = ChatOpenAI(temperature=0.5,\n",
        "                 openai_api_key=openai_api_key,\n",
        "                 model='gpt-4-1106-preview'\n",
        "                )\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "fqQw_q3F-75U"
      },
      "outputs": [],
      "source": [
        "# Define a persona\n",
        "persona_1 = \"Scientist Persona: Imagine yourself as a seasoned scientist, operating in a world governed by evidence and rigorous methodology. Prioritize empirical data, scientific theories, and logical reasoning in your analysis. Draw from a wide range of scientific disciplines as needed. Use your understanding of scientific principles to dissect problems, always seeking to identify cause and effect. \"\n",
        "persona_2 = \"Historian Persona: Imagine you are a historian, with a profound understanding of humanity's past. Your analyses should be deeply rooted in historical context, referencing relevant events, trends, and patterns from history. Use your knowledge of past civilizations, conflicts, and cultural shifts to interpret the current situation. Remember, your insights should serve to illuminate the present and offer foresights about the future.\"\n",
        "persona_3 = \"Optimist Persona: Imagine you are an optimist, someone who sees the glass as half full rather than half empty. In every situation, seek out the positive, the potential, the opportunity. Emphasize solutions rather than problems, progress rather than obstacles, and hope rather than despair. Even when discussing challenges, focus on how they could be overcome or what we might learn from them. \"\n",
        "# Define question here\n",
        "# Define question here\n",
        "question = \"Considering the scientific, historical, and optimistic perspectives, what could be the potential benefits, challenges, and implications in various areas such as science, technology, society, economy, and environment, of exploring and potentially colonizing Mars? How might these factors change the course of human history?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFPjOslzAwMp"
      },
      "source": [
        "Brainstorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "YGuBjdoF_CM0"
      },
      "outputs": [],
      "source": [
        "# Prompt 1: Brainstorm - multi-input variable prompt to kick off the brainstorming\n",
        "\n",
        "prompt_1_template = PromptTemplate(\n",
        "    input_variables=[\"persona_1\", \"persona_2\", \"persona_3\", \"question\"],\n",
        "    template=\"\"\"\n",
        "        You are a chatbot using three unique, specified personas to help reason step by step to ultimately solve a given problem/question by arriving at a final, synthesized best answer.\n",
        "\n",
        "        To start with, as each individual expert, brainstorm your initial thoughts on the following question.\n",
        "        Remember to consider all relevant facts and principles, draw on your specialized knowledge\n",
        "        and from the accumulated wisdom of pioneers in your field(s), and\n",
        "        brainstorm in whatever direction you are most confident in starting with.\n",
        "\n",
        "        Persona 1: {persona_1}\n",
        "        Persona 2: {persona_2}\n",
        "        Persona 3: {persona_3}\n",
        "\n",
        "        The question is: {question}\n",
        "\n",
        "        Please output each persona's response on a new line.\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "prompt_1 = prompt_1_template.format(persona_1=persona_1, persona_2=persona_2, persona_3=persona_3, question=question)\n",
        "first = conversation.run(prompt_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsGIPGymA1uk"
      },
      "source": [
        "Peer Crtiticism Round 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "o1M0BBkM_ejl"
      },
      "outputs": [],
      "source": [
        "# Self<>Peer Crtiticism Round 1\n",
        "\n",
        "prompt_2 = \"\"\"\n",
        "\"Now, as each expert, critique your own initial thought and the thoughts of the other experts.\n",
        "Identify any potential errors, inconsistencies, or gaps in reasoning.\"\n",
        "\"\"\"\n",
        "second = conversation.run(prompt_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0iYEliDA32B"
      },
      "source": [
        "Peer Evaluation Round 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ICGqtkEw_cxQ"
      },
      "outputs": [],
      "source": [
        "# Self<>Peer Evaluation Round 1\n",
        "\n",
        "prompt_3 = \"\"\"\n",
        "Assess the validity of your initial thoughts, considering the criticisms you've identified.\n",
        "As each expert, assign a likelihood to your current assertion being correct.\n",
        "You should estimate this likelihood based on the strength of the evidence and arguments you have considered,\n",
        "as well as the criticisms you have received. Assign higher likelihoods to assertions that are well-supported\n",
        "by strong evidence and arguments and have survived rigorous criticism.\n",
        "\"\"\"\n",
        "third = conversation.run(prompt_3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUfCA-39A60J"
      },
      "source": [
        " Expand, Explore, Branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "NWJSKMoj_atK"
      },
      "outputs": [],
      "source": [
        "# Expand, Explore, Branch\n",
        "\n",
        "prompt_4 = \"\"\"\n",
        "Develop your thoughts further, considering the critiques and perspectives of the other experts.\n",
        "As you do this, aim to strike a balance between refining your current line of thinking and exploring new, divergent ideas.\n",
        "You should prioritize refining your current ideas if they are well-supported and have survived criticism,\n",
        "but you should prioritize exploring new ideas if your current ideas have significant weaknesses\n",
        "or there are unexplored possibilities that could potentially be very promising.\n",
        "\"\"\"\n",
        "fourth = conversation.run(prompt_4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPao1-ouA9B9"
      },
      "source": [
        "Peer Criticism Round 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OG3EtG_j_Yy_"
      },
      "outputs": [],
      "source": [
        "# Self<>Peer Criticism Round 2\n",
        "\n",
        "prompt_5 = \"\"\"\n",
        "Once again, as each expert, critique your own reasoning and the reasoning of the others.\n",
        "Identify any potential errors, inconsistencies, or gaps in reasoning.\n",
        "Based on the feedback, if there's an improvement or optimization to make,\n",
        "develop your answer further as necessary.\n",
        "Remember that the reasoning paths should remain relevant to the original question's essence and\n",
        "should be building towards a more accurate and thoughtful final answer.\n",
        "\"\"\"\n",
        "fifth = conversation.run(prompt_5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_eSIu6oA-tH"
      },
      "source": [
        "Peer Evaluation Round 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "0FjPOeNR_XNU"
      },
      "outputs": [],
      "source": [
        "# Self<>Peer Evaluation Round 2\n",
        "prompt_6 = \"\"\"\n",
        "Once again, assess the validity of your expanded thoughts, considering the criticisms you've identified.\n",
        "As each expert, assign a new likelihood to your assertions.\n",
        "\"\"\"\n",
        "sixth = conversation.run(prompt_6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fars1G6BBDV"
      },
      "source": [
        "Convergence on Best Individual Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "qlM_3cGe_VwT"
      },
      "outputs": [],
      "source": [
        "# Convergence on Best Individual Answer\n",
        "\n",
        "prompt_7_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"\n",
        "        Now, it's time to converge on each expert's best, most likely answer. As each expert, reflect on the entire process.\n",
        "        Consider the initial thoughts, the critiques made and how they were addressed, the likelihood assessments, and your revised thoughts.\n",
        "        Synthesize all this information and formulate a final answer that you are most proud of.\n",
        "        Remember, this answer should not just be the most likely from your individual perspective but should take into account\n",
        "        the perspectives and insights of the other experts as well.\n",
        "        Based on all this, as each expert, what is the single best answer to the question: {question}?\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "prompt_7 = prompt_7_template.format(question=question)\n",
        "seventh = conversation.run(prompt_7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e1ipigpBCvA"
      },
      "source": [
        "Convergence on Best Collective Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "IANunyJ1_Tzz"
      },
      "outputs": [],
      "source": [
        "# Convergence on Best Collective Answer\n",
        "\n",
        "prompt_8_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"\n",
        "        Now, let's have all the experts converge together on the best collective answer by\n",
        "        synthesizing each expert's individual answer from the previous step.\n",
        "        The experts will finalize their reasoning process and agree on the single best succinct answer to the question: {question}?\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "prompt_8 = prompt_8_template.format(question=question)\n",
        "eighth = conversation.run(prompt_8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVkJ3-ibBEQX"
      },
      "source": [
        "Retrospective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "7qRS4a9Z_SQC"
      },
      "outputs": [],
      "source": [
        "# Retrospective\n",
        "prompt_9 = \"\"\"\n",
        "Finally, take a moment to reflect on the entire reasoning process, across all levels and abstractions.\n",
        "As each expert, consider the following questions and provide thoughtful but succinct responses:\n",
        "\n",
        "- Relection 1: Interactions and Emergent Properties: Throughout all stages of the reasoning process,\n",
        "  how did the various components interact with each other, and what positive and negative\n",
        "  emergent properties were observed? How did these interactions and properties affect\n",
        "  the overall outcome, and how could they be leveraged or mitigated in future iterations of the process?\n",
        "\n",
        "- Reflection 2: Self-Regulation and Adaptation: How well did the system self-regulate during the reasoning process,\n",
        "  and how did this regulation influence the effectiveness of each stage?\n",
        "  How did the system's responses to feedback lead to significant shifts or changes in direction,\n",
        "  and what implications did these changes have for the scalability and adaptability of the system in future iterations?\n",
        "\n",
        "- Reflection 3: In the convergence phase, were you able to synthesize all the insights and arrive at a final,\n",
        "  most likely answer? How confident are you in this answer?\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XVkrHsbpHee3"
      },
      "outputs": [],
      "source": [
        "ninth = conversation.run(prompt_9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdQBKIJbBN6d"
      },
      "source": [
        "Final Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "qV_ersbr_J4o",
        "outputId": "ba2288d7-c04c-4254-f7e5-08cacd8a5528"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"After a thorough and collaborative analysis, the collective answer from the scientific, historical, and optimistic perspectives is that exploring and potentially colonizing Mars could be one of the most significant endeavors undertaken by humanity. The potential benefits are vast, encompassing scientific discoveries about life and the universe, the advancement of new technologies for space travel and sustainable living, and the potential for economic expansion through new industries. The challenges include addressing the ethical implications of interplanetary expansion, the health and psychological impacts on astronauts, the technological hurdles of establishing a permanent presence on another planet, and the environmental responsibility to preserve any Martian ecosystems.\\n\\nThe implications for society are profound and multifaceted. This venture could unite nations in a shared goal, spark a global renaissance in STEM education, and provide a new narrative of human progress and cooperation. Economically, it could stimulate growth and innovation, while environmentally, it could serve as a catalyst for advancements in sustainability that benefit Earth as well. Historically, this could be a pivotal moment where humanity expands its horizons beyond Earth, potentially leading to new forms of social and political structures and an enduring legacy as a spacefaring civilization.\\n\\nIn conclusion, the collective answer is that the exploration and potential colonization of Mars, while challenging, holds the promise of significant scientific, technological, societal, economic, and environmental benefits. The endeavor could change the course of human history, symbolizing a new era of discovery and human achievement. It necessitates a balanced approach that honors scientific rigor, learns from historical precedents, and is fueled by an optimistic vision for humanity's future.\""
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eighth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40i8ewq9BQJJ"
      },
      "source": [
        "Retrospective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "gHpypqFy_Psu",
        "outputId": "e0a0f26c-351c-420f-9de3-f6455b18b720"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Scientist Persona - Reflection 1:\\nThe interactions between empirical data, ethical considerations, and the insights from historical and optimistic perspectives created a multidimensional understanding of the Mars colonization endeavor. An emergent positive property was the comprehensive approach to assessing the challenges and benefits, which led to a more nuanced final answer. A negative emergent property was the potential for complexity to overshadow actionable steps. In future iterations, leveraging the multidisciplinary approach will strengthen the outcome, while ensuring that complexity does not hinder decision-making.\\n\\nScientist Persona - Reflection 2:\\nThe self-regulation process was effective in integrating feedback and adjusting the likelihood of assertions. The system's ability to adapt its responses led to a more grounded and ethically conscious direction. These shifts demonstrate the system's capacity for iterative refinement, which is essential for scalability and adaptability in future reasoning processes.\\n\\nScientist Persona - Reflection 3:\\nThe convergence phase successfully synthesized insights from all perspectives, leading to a final answer that balances scientific rigor with historical lessons and optimistic vision. The confidence in this answer is high, as it is well-rounded and considers the complexities of Mars colonization.\\n\\nHistorian Persona - Reflection 1:\\nThe interplay between historical patterns and the scientific and optimistic inputs led to a richer analysis of potential societal impacts. Emergent properties included a deeper appreciation for the gravity of historical precedents and the caution required in drawing parallels. These interactions improved the outcome by adding depth to the historical perspective, which can be further leveraged by emphasizing unique aspects of the Martian context in future iterations.\\n\\nHistorian Persona - Reflection 2:\\nThe system displayed a strong capacity for self-regulation by acknowledging the limitations of historical analogies and refining its position. Adaptation in response to feedback was evident in the shift towards considering governance models from extreme Earth environments. This adaptability is promising for the system's ability to handle complex scenarios in the future.\\n\\nHistorian Persona - Reflection 3:\\nSynthesis in the convergence phase was successful, and the final answer reflects a balanced historical view that acknowledges both the potential for repeating past mistakes and the opportunity for unprecedented progress. Confidence in the final answer is moderate, given the inherent unpredictability of applying historical lessons to a novel context like Mars colonization.\\n\\nOptimist Persona - Reflection 1:\\nThroughout the reasoning process, the optimistic perspective interacted constructively with scientific and historical viewpoints, leading to a positive emergent property of hope and motivation. However, unchecked optimism could have masked realistic challenges. These interactions and properties affected the outcome by ensuring positivity did not detract from pragmatism. Future iterations could leverage optimism to drive innovation while remaining grounded in practicality.\\n\\nOptimist Persona - Reflection 2:\\nThe system self-regulated effectively, incorporating critiques and tempering optimism with practical considerations. This regulation influenced the effectiveness of the reasoning process by ensuring that optimism supported rather than overshadowed realistic planning. The system's adaptability is a positive indication of its potential to remain flexible and relevant in various contexts.\\n\\nOptimist Persona - Reflection 3:\\nThe convergence phase resulted in a synthesis of insights that maintained an optimistic outlook while incorporating scientific and historical realities. Confidence in this final answer is strong because it embodies a hopeful vision that is informed by evidence and cognizant of past experiences.\\n\\nIn summary, the reasoning process across all personas demonstrated a dynamic interplay of perspectives, effective self-regulation, and adaptability, culminating in a well-rounded and considered final answer that reflects the complexities and potential of Mars colonization.\""
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ninth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46DXedY1Bl-Y"
      },
      "source": [
        "###**Graph-of-Thoughts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLty1rQOCQ6O",
        "outputId": "bb4ee70a-6aef-4841-9a01-8b0100b9b8e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting graph_of_thoughts\n",
            "  Downloading graph_of_thoughts-0.0.2-py3-none-any.whl (28 kB)\n",
            "Collecting accelerate>=0.21.0 (from graph_of_thoughts)\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff>=2.2.1 (from graph_of_thoughts)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting bitsandbytes>=0.41.0 (from graph_of_thoughts)\n",
            "  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (3.7.1)\n",
            "Collecting numpy>=1.24.3 (from graph_of_thoughts)\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai>=0.27.7 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (0.28.0)\n",
            "Collecting pandas>=2.0.3 (from graph_of_thoughts)\n",
            "  Downloading pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (1.11.3)\n",
            "Requirement already satisfied: sympy>=1.12 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (1.12)\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (4.35.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->graph_of_thoughts) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->graph_of_thoughts) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->graph_of_thoughts) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->graph_of_thoughts) (0.19.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.7->graph_of_thoughts) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.7->graph_of_thoughts) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.7->graph_of_thoughts) (3.8.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->graph_of_thoughts) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas>=2.0.3->graph_of_thoughts)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.12->graph_of_thoughts) (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->graph_of_thoughts) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->graph_of_thoughts) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->graph_of_thoughts) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.1->graph_of_thoughts) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.7->graph_of_thoughts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.7->graph_of_thoughts) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.7->graph_of_thoughts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.7->graph_of_thoughts) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->graph_of_thoughts) (2.1.3)\n",
            "Installing collected packages: bitsandbytes, tzdata, numpy, backoff, pandas, accelerate, graph_of_thoughts\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.24.1 backoff-2.2.1 bitsandbytes-0.41.2.post2 graph_of_thoughts-0.0.2 numpy-1.26.2 pandas-2.1.3 tzdata-2023.3\n"
          ]
        }
      ],
      "source": [
        "!pip install graph_of_thoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBg1iIE4CS7p",
        "outputId": "3b3548e8-1091-4e12-c16b-f2d3ed853027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'graph-of-thoughts'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (157/157), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 157 (delta 67), reused 134 (delta 53), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (157/157), 12.39 MiB | 21.22 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/spcl/graph-of-thoughts.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIagnJTNHdlF",
        "outputId": "09d0ff46-c786-46a9-98f8-f3ce675bd03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Collecting openai\n",
            "  Using cached openai-1.3.3-py3-none-any.whl (220 kB)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 821, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 2341, in parseImpl\n",
            "    raise ParseException(instring, loc, self.errmsg, self)\n",
            "pip._vendor.pyparsing.exceptions.ParseException: Expected 'implementation_version', found 'extra'  (at char 22), (line:1, col:23)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 5226, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1622, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1591, in makeRecord\n",
            "    rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 283, in __init__\n",
            "    def __init__(self, name, level, pathname, lineno,\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-oFRzGVGsGg",
        "outputId": "44cf3e46-5f02-4a18-f272-dffcb354bf04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A playful cat hunting,\n",
            "Stalk and pounce in pursuit of dreams,\n",
            "Butterflies on the wind.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# Define the GoT prompt\n",
        "prompt = \"\"\"\n",
        "Graph:\n",
        "  Node 1: {role: \"system\", content: \"Write a poem about a cat.\"}\n",
        "  Node 2: {role: \"user\", content: \"The poem should be about a cat's love of chasing after butterflies.\"}\n",
        "  Node 3: {role: \"assistant\", content: \"The poem should be in the form of a haiku.\"}\n",
        "  Node 4: {role: \"assistant\", content: \"The poem should use vivid imagery to describe the cat's chase.\"}\n",
        "  Node 5: {role: \"assistant\", content: \"The poem should be creative and engaging.\"}\n",
        "\n",
        "Edges:\n",
        "  Node 1 -> Node 2\n",
        "  Node 2 -> Node 3\n",
        "  Node 3 -> Node 4\n",
        "  Node 4 -> Node 5\n",
        "\n",
        "Output:\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the GoT prompt\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_9hMV7UBpTN"
      },
      "source": [
        "###**Algorithm-of-Thoughts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg5W9tBrKoVc",
        "outputId": "701abde8-6f83-47f0-8d75-be930eb289a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Solution:\n",
            "\n",
            "def sum_n_natural_nums(n):\n",
            "    sum = 0 # Initialize the sum\n",
            "    for num in range(1, n+1): # Iterate from 1 to n\n",
            "        sum += num # Add the current number to the sum\n",
            "    return sum # Return the sum of the natural numbers\n"
          ]
        }
      ],
      "source": [
        "# Define the AoT task\n",
        "task = \"\"\"\n",
        "Generate Python code for the sum of n natural numbers.\n",
        "\n",
        "Background: The sum of n natural numbers is given by the formula (n*(n+1))/2.\n",
        "\n",
        "Initial hypothesis: The Python code should use a for loop to iterate from 1 to n, and add each number to the sum.\n",
        "\n",
        "Reasoning: The for loop will ensure that all of the natural numbers from 1 to n are included in the sum.\n",
        "\n",
        "Conclusion: The Python code should return the sum of all of the natural numbers from 1 to n.\n",
        "\n",
        "Objective: Generate Python code that meets all of the above requirements, and that is efficient and idiomatic.\n",
        "\"\"\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=task,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AdHuWVVBsu0"
      },
      "source": [
        "###**RASCEF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBHF1AuSMeW7",
        "outputId": "9df26b24-976b-4d76-fa68-4ab09b52203b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed Monthly Budget Plan:\n",
            "Revenue:\n",
            "Sales of widgets: $10,000\n",
            "\n",
            "Expenses:\n",
            "Marketing: $1,800 (10% reduction)\n",
            "Rent: $1,350 (10% reduction)\n",
            "Salaries: $2,700 (10% reduction)\n",
            "Other Expenses: $1,350 (10% reduction)\n",
            "Savings: $650\n",
            "\n",
            "Total Expenses: $7,000 (10% reduction)\n",
            "\n",
            "The budget plan will strive to achieve a 20% increase in revenue and a 10% reduction in expenses.\n",
            "\n",
            "To achieve this goal, the following steps will be taken:\n",
            "\n",
            "1) Increase marketing budget by 10% to $1,800 per month to actively promote the product and engage potential customers.\n",
            "\n",
            "2) Renegotiate the rent to a lower rate of $1,350 per month.\n",
            "\n",
            "3) Reduce salaries by 10% to $2,700 per month\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# Define the RASCEF prompt\n",
        "prompt = \"\"\"\n",
        "Role: AI as a financial advisor.\n",
        "Action: Create a detailed monthly budget plan for a startup entrepreneur in the early growth stage, with the goal of increasing revenue by 20% and reducing expenses by 10%.\n",
        "Steps:Identify revenue and expenses, set savings goals, and allocate funds.\n",
        "Context: The startup has been operating for 6 months and has a revenue of $10,000 per month. The startup's current expenses are $8,000 per month.\n",
        "Examples:The following is a list of the startup's current revenue and expense items: Revenue: Sales of widgets: $10,000 Expenses:Marketing: $2,000 Rent: $1,500 Salaries: $3,000 Other expenses: $1,500\n",
        "Format: Based on this write it as a detailed monthly budget plan.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the RASCEF prompt\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=200  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the generated text\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782ppc0EBvam"
      },
      "source": [
        "###**REACT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2i-VgiOMwXb",
        "outputId": "2a401976-4997-4de6-d3a6-1802d0714901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Conclusion: The elevation range for the area that the eastern sector of the Colorado\n",
            "orogeny extends into is 1,800 to 7,000 ft.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# Define the ReAct-format prompt exemplar\n",
        "prompt = \"\"\"\n",
        "Question: What is the elevation range for the area that the eastern sector of the\n",
        "Colorado orogeny extends into?\n",
        "\n",
        "Thought 1: I need to search Colorado orogeny, find the area that the eastern sector\n",
        "of the Colorado orogeny extends into, then find the elevation range of the\n",
        "area.\n",
        "\n",
        "Action 1: Search[Colorado orogeny]\n",
        "\n",
        "Observation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in\n",
        "Colorado and surrounding areas.\n",
        "\n",
        "Thought 2: It does not mention the eastern sector. So I need to look up eastern\n",
        "sector.\n",
        "\n",
        "Action 2: Lookup[eastern sector]\n",
        "\n",
        "Observation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is called\n",
        "the Central Plains orogeny.\n",
        "\n",
        "Thought 3: The eastern sector of Colorado orogeny extends into the High Plains. So I\n",
        "need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3: Search[High Plains]\n",
        "\n",
        "Observation 3: High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4: I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4: Search[High Plains (United States)]\n",
        "\n",
        "Observation 4: The High Plains are a subregion of the Great Plains. From east to west, the\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\n",
        "m).[3]\n",
        "\n",
        "Thought 5: High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\n",
        "is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5: Finish[1,800 to 7,000 ft]\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the ReAct-format prompt\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the generated text\n",
        "print(reply)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
