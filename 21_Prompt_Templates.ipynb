{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt pattern templates**"
      ],
      "metadata": {
        "id": "qWfQs-oJXUnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "feK11022X4Vp",
        "outputId": "86cfd21e-b9e0-4e8a-cf85-3e74c927aee3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.3.3\n",
            "    Uninstalling openai-1.3.3:\n",
            "      Successfully uninstalled openai-1.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = 'open api key'"
      ],
      "metadata": {
        "id": "WJ24m_K_Iqk3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Persona Pattern Example**"
      ],
      "metadata": {
        "id": "8gaGkkRRXsW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this scenario, we'll create a persona for a character named \"Sarah\" who is a biology student. We'll ask her to write an essay about photosynthesis."
      ],
      "metadata": {
        "id": "6ctQXZmMXX0p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q8mMEEuXUHE",
        "outputId": "f106d9ff-3a0a-43a2-e130-7126d8165e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Photosynthesis is the process by which plants convert energy from the sun into carbohydrates. Through the process of photosynthesis, plants are able to capture and store solar energy for use later as food and energy. The process is critical for the health of the environment, as it is responsible for providing oxygen for the majority of the Earth’s inhabitants.\n",
            "\n",
            "The process of photosynthesis begins when light energy from the sun is absorbed by a plant’s chloroplasts, chlorophyll molecules of which are located inside the photosynthesizing cells of the plant. This energy is then used to breakup water molecules into oxygen and hydrogen. The hydrogen combines with carbon dioxide, which is taken in by the plant through its leaves, to create a variety of carbohydrates, such as glucose and sucrose. This newly-created energy is then stored in the plant for use later.\n",
            "\n",
            "Photosynthesis is an extremely important process for the environment. For one, photosynthesis helps to create an atmosphere that consists of oxygen, necessary for humans and animals to live. In addition, photosynthesis helps to maintain the Earth’s natural balance of carbon dioxide which is crucial for regulating global temperatures. Photosynthesis is also responsible for producing the food that sustains life on earth, as the carbohydrates produced are able to be utilized by any living creature. \n",
            "\n",
            "The importance of photosynthesis goes beyond merely providing essential elements for life: it can also be used to help fight climate change. Scientists are currently looking into ways to increase the efficiency of photosynthesis so it can capture more sunlight and store more carbon dioxide than it currently does. This extra energy/carbon dioxide capture could help make a substantial contribution to fighting climate change, as more of the carbon dioxide in the atmosphere could be neutralized. \n",
            "\n",
            "In conclusion, photosynthesis is an incredibly important process in the world. Not only does it provide necessary oxygen to living creatures, but it also helps to create a balanced environment and provides essential food sources. Additionally, research is being done to aid in the fight against climate change. It is clear that photosynthesis is a key factor in the health of the environment.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"You are Sarah, a biology student who is passionate about plants and photosynthesis. Write an informative essay about the process of photosynthesis, its importance, and its impact on the environment.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500  # Adjust as needed\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Audience Persona Pattern**"
      ],
      "metadata": {
        "id": "7svV3fOZZm6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we'll define a persona and a specific audience for the prompt. Here's an example using a persona for a biology teacher, and the audience is a group of high school students:\n",
        "\n",
        "This prompt instructs the AI to take on the persona of a biology teacher and provide an explanation suitable for high school students, keeping their age and comprehension level in mind. It's designed to create content tailored to a specific audience."
      ],
      "metadata": {
        "id": "mzD_iQUaZrbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"As a high school biology teacher addressing a class of students, explain the concept of cellular respiration in a way that is easy for them to understand. Use simple language and provide relatable examples to make the topic engaging.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTxFFPD5ZwVA",
        "outputId": "f27a852c-63b9-4722-ba6c-3c21c8ce4523"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Cellular respiration is like a machine that your cell uses to get energy! The machine runs on fuel which in this case is called glucose, and oxygen is the 'air' that helps the machine work. To put it simply, cellular respiration is the process of taking glucose, adding oxygen to it, and breaking it down in order to create energy, like a car burning gasoline. Think of glucose and oxygen as the raw ingredients and the energy produced as a product that can be used to do things like move, think, and grow. \n",
            "\n",
            "Using an analogy, you can think of it like a bakery that uses flour, sugar, and other ingredients to bake a cake. The baker takes these ingredients, combines them together, bakes the cake, and then ends up with a delicious treat! It's the same concept with cellular respiration - the cell takes in glucose and oxygen, combines them together, and then produces energy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question Refinement Pattern**"
      ],
      "metadata": {
        "id": "1PtMOocHaqCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we start with a broad question and refine it step by step to reach a specific answer.\n",
        "\n",
        "Question Refinement Pattern Example:"
      ],
      "metadata": {
        "id": "5I65oeF1avpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Let's explore the world of astronomy. Start by explaining the concept of a 'black hole.' First, provide a general overview of what a black hole is, and then dive deeper to explain how the gravitational collapse of a massive star leads to the formation of a black hole. Finally, discuss the implications of black holes on the fabric of space-time.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-TmoHQTXv5g",
        "outputId": "4bacf20f-ad64-44b8-ac2c-99ffc1074641"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "A black hole is a region of space which has a gravitational field so strong that nothing, not even light, can escape from it. It is believed to be formed when a large star dies, collapses inwards and becomes so dense and its gravitational field so powerful that no external force can prevent it from further collapse. These collapsed stars have a gravitational field strong enough to bend the path of light and form a spacetime singularity -an event horizon- from which nothing, including light, can escape. \n",
            "\n",
            "The gravitational collapse of a massive star is a process which occurs over millions of years, beginning as the star is nearing the end of its life. Having used all of its nuclear fuel, the star can no longer support itself against its own gravity and it collapses. This collapse continues until the core of the star is compressed into a single point, known as a singularity, surrounded by an invisible boundary called an event horizon. The event horizon marks the point beyond which nothing can escape, not even light.\n",
            "\n",
            "The formation of a black hole can have drastic implications on the fabric of space-time. These implications are still being studied, but one major effect is that the space-time within the event horizon of a black hole is curved inwards around the singularity. This means that time moves more slowly near a black hole and that it is possible for an observer on the brink of the event horizon to observe the outside world in fast-forward. Additionally, a black hole’s immense gravitational pull can cause warps and distortions in nearby space-time, resulting in the bending of light around the black hole and also producing gravitational waves. Lastly, the presence of a black hole can lead to the formation of entire galaxies, as the black hole's gravity pulls in gas and dust which then coalesce to form new stars.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Cognitive Verifier Pattern**"
      ],
      "metadata": {
        "id": "1Yu-2yuTbcfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we ask the AI to explain its reasoning or provide evidence for a given answer.\n",
        "\n",
        "This prompt instructs the AI to provide a detailed explanation and evidence for the assertion that climate change is mainly caused by human activities. It's designed to make the AI elaborate on its reasoning."
      ],
      "metadata": {
        "id": "bl_KvtHgbSbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Explain how you arrived at the conclusion that climate change is primarily caused by human activities. Provide detailed evidence, scientific reasoning, and examples to support this claim.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hGRzJ4ua322",
        "outputId": "87e6bb60-11ed-4eeb-811b-52ccf7b2bc4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Climate change is primarily caused by human activities due to the release of greenhouse gases into the atmosphere primarily from burning fossil fuels. These gases build up in the atmosphere, trapping heat and raising temperatures. In fact, more than 90 percent of the warming seen over the past century is attributable to human activities.\n",
            "\n",
            "Evidence for this claim comes from scientific research. Since the industrial revolution, the global atmospheric concentration of carbon dioxide (CO2), the main greenhouse gas, has been steadily increasing due to the burning of fossil fuels. There are other human activities that contribute to the increase in atmospheric greenhouse gases, such as deforestation, which reduces the natural sinks for carbon and other gases.\n",
            "\n",
            "The Intergovernmental Panel on Climate Change (IPCC) outlines the current scientific understanding of why human activities are to blame for the current climate crisis. They state that the amount of greenhouse gases released from human activities is greater than the natural exchange of these gases, resulting in a net accumulation in the atmosphere, and trapping more heat. The IPCC’s assessment is drawn from series of evidence, from climate models to paleo-climatology, and demonstrates that throughout all of known human history, only now has the Earth seen such rapid and dramatic changes in climate.\n",
            "\n",
            "The greenhouse gas emissions from human activities are estimated to account for around four fifths of the global increased radiative forcing during the industrial period. For instance, a 2013 study conducted by The University of East Anglia and the University of Exeter found that around 97 percent of the observed energy imbalance was due to human activities. This type of research provides strong evidence that climate change is primarily caused by human activities. \n",
            "\n",
            "In short, climate change is primarily caused by human activities due to the emissions of greenhouse gases. This is backed up by numerous climate and energy modeling studies as well as paleo-climatological evidence, demonstrating the dramatic changes observed over the past century.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Flipped Interaction Pattern**"
      ],
      "metadata": {
        "id": "pLBXa47tcp-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we invert the role of the model and the user, asking the model to generate content as if it were a different entity.\n",
        "\n",
        "In this example, we take on the persona of a famous science fiction author and request the AI to generate the opening paragraph of a science fiction novel, setting the stage for the discovery of an alien civilization on a distant planet."
      ],
      "metadata": {
        "id": "RU-DfL0zcuo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"You are a famous science fiction author, and you've been asked to write the opening paragraph of your next bestselling novel. The story should revolve around the discovery of a hidden alien civilization on a distant planet. Start with a captivating scene that draws readers into this exciting adventure.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DGjDfSjbtvu",
        "outputId": "fe3dd630-3e9b-44b7-fd24-7e8523a8f235"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The view through the cryogenic glasspod was breath-taking. Ahead of the spacecraft lied a blue planet like no other, reflecting the glimmering rays of the distant sun and whispering along the reaches of an unknown universe. Captain Amanda Thomas and her crew had spent weeks hurtling through their starry exile, and now the alien metropolis they'd journeyed so far to discover lay before their very eyes. The long-lost home of a distant civilization, tucked away on the other side of the cosmos, seemed to be nothing more than a myth–until now. But as their ship descended toward the planet's surface, a strange excitement began to build among the crew as they wondered what secrets, wonders, and mysteries may lay ahead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Zero-shot Prompting Pattern**"
      ],
      "metadata": {
        "id": "F460qFEVdH-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we use a single prompt to get the model to perform a task without any additional examples or context.\n",
        "\n",
        "In this example, we provide a straightforward request for the AI to translate an English sentence into German without providing any additional context or examples."
      ],
      "metadata": {
        "id": "feT0Uhq_dObi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Translate the following English text into German: 'The quick brown fox jumps over the lazy dog.'\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vTLsdUdc87h",
        "outputId": "a4d9f91a-4407-4801-b27d-a1acb73293f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Der schnelle braune Fuchs springt über den faulen Hund.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**One-shot Prompting Pattern**"
      ],
      "metadata": {
        "id": "sy7MrdaidiYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we provide a single example to help the model perform a task.\n",
        "\n",
        "In this example, we request the AI to translate an English sentence into French, and we provide a single example to assist the model in understanding the task."
      ],
      "metadata": {
        "id": "StJiRk2cdlV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Translate the following English text into French: 'Hello, how are you? My name is Dana'. Example of English to French translation: 'English': 'I love my husband.' 'French': 'J'aime mon mari.'\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYdNG7C6daeq",
        "outputId": "96f0e419-a796-4941-ceb2-4f70b64c5224"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Salut, comment allez-vous ? Mon nom est Dana.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Few-shot Prompting Pattern**"
      ],
      "metadata": {
        "id": "tTkf_yTze37S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we provide a few examples to help the model understand and perform a task.\n",
        "In this example, we're instructing the AI to generate a short story involving a detective solving a murder case, and we provide a few examples to inspire the story."
      ],
      "metadata": {
        "id": "26o5-C98e32i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Generate a short story about a detective solving a mysterious murder case. Use the following examples as inspiration: 'Detective Smith received an anonymous letter,' 'The murder weapon was a rare antique dagger,' and 'The victim had a secret life.'\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dz2jW2Zenzq",
        "outputId": "cd26828e-20c2-4cd7-ca33-441273677e52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Detective Smith received an anonymous letter that drew her interest. It claimed to know the identity of the perpetrator of a recent murder case. Smith investigated further and eventually uncovered the motive behind the crime: a rare antique dagger used to do the deed and no fingerprints whatsoever.\n",
            "\n",
            "Despite the lack of clues, Smith took up the challenge. She began to investigate the part of the crime that seemed most suspicious: the victim's secret life.\n",
            "\n",
            "It turned out the victim was actually a high-profile political figure with a deep connection to a powerful criminal organization. The suspect Smith was tipped off to was one of the victim's closest associates and was also connected to the organization.\n",
            "\n",
            "Smith's investigation revealed further links between the suspect and the criminal organization, which acquired the rare antique dagger not long before the crime took place. Detecting was almost certain that the murder weapon was the same.\n",
            "\n",
            "After days of digging, the detective took what she had found to the District Attorny and successfully secured an arrest warrant for the suspect. With the help of her anonymous tip, Smith had uncovered the truth of a murderer's identity. The victim's family could now find some closure in knowing that justice had been served.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Chain of Thought Prompting Pattern**"
      ],
      "metadata": {
        "id": "LypZhkfqfTzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this pattern, we instruct the model to generate content in a sequential, interconnected manner, building upon previous information or ideas.\n",
        "\n",
        " In this example, the AI is guided to write a travel blog post in a sequential manner, starting with the arrival in Paris and then progressing to different aspects of the journey through Europe."
      ],
      "metadata": {
        "id": "UsOJRIkHfTqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Imagine you are a travel blogger writing about your journey through Europe. Start by describing your arrival in Paris. Then, recount your experiences with the local cuisine, the breathtaking architecture, and the vibrant nightlife. Conclude by sharing your plans for the next destination, Venice.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxwt6am7fKth",
        "outputId": "1ff3315b-65c6-4961-95e9-855a11bd1589"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "My first stop in Europe was undoubtedly a special one; the ever-romantic city of Paris. Arriving late in the evening, I was immediately struck by the beauty of the city. Everywhere I looked, Paris was alive with energy and positivity despite the dusky night sky. \n",
            "\n",
            "The next few days were nothing short of remarkable - from exploring the crowded but charming markets to try the delicious local cuisine to capturing the beauty of the breathtaking architecture that lined the streets. I also had the pleasure of experiencing an evening in a cafe along the River Seine, surrounded by the sound of contemporaries playing light jazz in the background - it was the perfect way to experience and fall in love with the Parisian culture. \n",
            "\n",
            "At night the city transformed into a bright and lively hub of creation with concerts galore and street performers dotting the entire city. My experiences in Paris were unique and nothing short of exquisite, and it was the perfect way to kick-off my journey through Europe.\n",
            "\n",
            "As the trip continues, I am now looking forward to my next destination, Venice. Built upon one hundred and seventeen small islands and connected through canals, there is no doubt Venice will prove to be a magical and a truly unique experience. I am filled with excitement to walk the narrow streets and picturesque pathways, take a gondola ride and taste the best of Mediterranean cuisine. From Paris to Venice, I can't contain my enthusiasm for the adventure that lies ahead!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tree of Thought Prompting Pattern**"
      ],
      "metadata": {
        "id": "zeQ1ryzkf1bC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this pattern, we instruct the model to generate content that explores multiple branches or facets of a topic.\n",
        "\n",
        " In this example, we guide the AI to explore different branches of the topic of renewable energy sources, including solar energy, wind energy, and hydropower, with an emphasis on benefits, environmental impact, efficiency, and challenges."
      ],
      "metadata": {
        "id": "3gKaHCGjf0EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Dive into the topic of renewable energy sources. Begin by providing an overview of solar energy and its benefits. Then, discuss wind energy, its environmental impact, and efficiency. Finally, explore the future prospects of hydropower and the challenges it faces in sustainable energy production.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJmIRXrWffJz",
        "outputId": "5c51fd87-a77a-4c9d-ccbf-83b110f30442"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Solar energy is energy from the sun that is converted into thermal or electrical energy. It is the most abundant renewable energy source on the planet and is capable of powering up entire communities. The main benefits of solar energy include its cost savings, reliability, sustainability, and effect on the environment.\n",
            "\n",
            "In terms of cost savings, solar energy can be used to offset expensive energy costs, as it generates free energy from the sun. Solar energy has the added benefit of being reliable since the sun rises every day and can still produce energy even on cloudy days. In terms of sustainability, solar energy produces no emissions or pollutants and contributes significantly to reducing the environmental footprint of energy production. In terms of the environment, the use of solar energy helps to reduce greenhouse gas emissions and global warming, leading to a healthier planet.\n",
            "\n",
            "Wind energy is another form of renewable energy that harnesses the power of the wind to generate electricity. Wind turbines convert kinetic energy from the wind into electrical energy that can be used as an alternate power source. The environmental impact of wind energy is largely positive, as they generate no emissions or pollutants. Wind turbines are also very efficient, typically converting around 45-50% of wind energy into electricity.\n",
            "\n",
            "Finally, the future potential of hydropower is immense, as it is a clean and renewable energy source. Hydropower harnesses the energy from flowing water to generate electricity. It has been a mainstay of renewable energy production for decades, but recent technological advances in turbine and dam technologies have made it exponentially more efficient and powerful. Despite its cost advantages, hydropower is not without its challenges when it comes to sustainable energy production. The construction of large dams can cause a disruption of local landscapes, natural habitats, and ecosystems. In addition, the fluctuations in water levels within the dams can affect the availability of water for local communities. Another challenge facing hydropower is the potential for increased flooding due to the construction of dams and reservoirs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Graph of Thought Prompting Pattern**"
      ],
      "metadata": {
        "id": "kl5AdbYxhdrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we instruct the model to generate content that presents a complex, interconnected web of ideas or concepts.\n",
        "\n",
        "In this example, we guide the AI to create a comprehensive visualization or description of the interconnected ideas related to artificial intelligence (AI) and its applications across various fields, including the ethical and privacy considerations."
      ],
      "metadata": {
        "id": "PUA8oaFmgdVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Explore the concept of artificial intelligence (AI) and its impact on various fields. Create a visual 'mind map' of AI applications, from healthcare to finance, and show how AI intersects with ethics, privacy, and innovation.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dr-PvqfgFDT",
        "outputId": "c8fc16f3-9905-4c7c-adb9-00f574161426"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "AI Mind Map:\n",
            "\n",
            "Healthcare: \n",
            "• Automated Diagnostics (AI-driven diagnosis) \n",
            "• Automated Care (AI-driven medical decisions) \n",
            "• Robot-Assisted Surgery \n",
            "Ethics: \n",
            "• AI and Algorithmic Bias \n",
            "• Data Privacy \n",
            "• Automated Moral Decision-Making \n",
            "Finance: \n",
            "• Automated Investing \n",
            "• AI-driven Credit Scoring \n",
            "• AI-driven Fraud Detection \n",
            "Innovation: \n",
            "• Automation of Research and Development \n",
            "• Automation of Manufacturing Processes \n",
            "• Autonomous Vehicles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Prompt Chaining Pattern**"
      ],
      "metadata": {
        "id": "dv8yGDy1h6Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern, we chain a sequence of prompts to build a conversation or story.\n",
        "\n",
        "In this example, we create a sequence of prompts that simulate a conversation between an investigative journalist and a scientist. The prompts are chained to elicit a comprehensive discussion about the scientist's discovery in renewable energy."
      ],
      "metadata": {
        "id": "6qNbgMboh6Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"You are an investigative journalist interviewing a scientist. Start by asking the scientist about their groundbreaking discovery in renewable energy. Ask a series of connected questions. After they explain, ask about the potential applications and impact on the environment. Finally, inquire about the challenges they faced during their research.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500\n",
        ")\n",
        "# Extract and print the response\n",
        "generated_text = response.choices[0].text\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7dOOmu1htQa",
        "outputId": "0d6f5a3a-8d5e-4c26-f970-b42d71ed4b2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Question 1: Can you tell me about your groundbreaking discovery in renewable energy?\n",
            "\n",
            "Question 2: What gave you the idea to explore this field of research?\n",
            "\n",
            "Question 3: How have your findings advanced renewable energy technologies?\n",
            "\n",
            "Question 4: What potential applications do you see for your discovery?\n",
            "\n",
            "Question 5: How do you think your research will impact the environment?\n",
            "\n",
            "Question 6: What were the biggest challenges you faced during your research?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ReAct Prompting Pattern**"
      ],
      "metadata": {
        "id": "hPs82LfGjH4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReAct Prompting Pattern is a structured way of guiding a language model through a sequence of actions to perform a specific task or answer a question. It involves breaking down the task into a series of \"Thoughts\" and \"Actions,\" where \"Thoughts\" represent your mental steps, and \"Actions\" represent the model's tasks in response to our thoughts.\n",
        "\n",
        "Here's how the ReAct Prompting Pattern works:\n",
        "\n",
        "Question: You start with a clear question or task that you want the model to answer or perform.\n",
        "\n",
        "Thoughts: These are your internal thought processes, where you outline the logical steps you'd take to answer the question or complete the task. Each thought represents a mental action or step you'd naturally follow.\n",
        "\n",
        "Actions: These are the specific instructions to the model based on your thoughts. You tell the model what to do at each step, such as searching for information, summarizing findings, or performing calculations.\n",
        "\n",
        "Observations: After each action, you might include observations or notes about the information or results obtained from that action.\n",
        "\n",
        "Finish: When you've gone through all the thoughts and actions and have obtained the necessary information or answer, you conclude the process.\n",
        "\n",
        "This pattern is useful for tasks that require a logical sequence of actions or research, allowing us to guide the model through the steps needed to reach a conclusion or generate a comprehensive response.\n",
        "\n",
        "The ReAct Prompting Pattern provides a structured way to interact with the model, which can be particularly valuable for complex research, problem-solving, or investigative tasks. It ensures that the model processes the information methodically, similar to how a human would approach the same task."
      ],
      "metadata": {
        "id": "Y629_Y4VjGTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define a ReAct-format prompt for researching climate change effects\n",
        "prompt = \"\"\"\n",
        "Question: What are the environmental consequences of climate change in the Arctic region?\n",
        "\n",
        "Thought 1: To answer this question, I need to first understand the impacts of climate change in the Arctic.\n",
        "\n",
        "Action 1: Research the impacts of climate change in the Arctic region.\n",
        "\n",
        "Observation 1: Climate change in the Arctic is causing rapid ice melt, leading to rising sea levels and threatening wildlife.\n",
        "\n",
        "Thought 2: Let's delve deeper into the impact on wildlife.\n",
        "\n",
        "Action 2: Investigate the effects of climate change on Arctic wildlife.\n",
        "\n",
        "Observation 2: Melting ice disrupts polar bear habitats, and declining sea ice affects seals and other marine life.\n",
        "\n",
        "Thought 3: This information is essential, but I also want to know how climate change impacts the indigenous communities.\n",
        "\n",
        "Action 3: Research the effects of climate change on Arctic indigenous communities.\n",
        "\n",
        "Observation 3: Indigenous communities face challenges related to food security, infrastructure, and cultural preservation due to climate change.\n",
        "\n",
        "Thought 4: Now, I need to summarize all these findings into a cohesive response.\n",
        "\n",
        "Action 4: Summarize the environmental consequences of climate change in the Arctic region.\n",
        "\n",
        "Observation 4: The Arctic is experiencing ice melt, threatening wildlife like polar bears and seals. Indigenous communities are also affected, with issues related to food security and cultural preservation.\n",
        "\n",
        "Thought 5: I can now use this information to provide a comprehensive response.\n",
        "\n",
        "Action 5: Respond with a summary of the environmental consequences of climate change in the Arctic.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the ReAct-format prompt\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the generated response\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtMWAMzZiJ5R",
        "outputId": "aa397fd7-6803-4304-f592-90d9acaccdb7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer: Climate change in the Arctic is causing rapid ice melt, leading to rising sea levels and threatening wildlife, such as polar bears and seals. Indigenous communities are also facing issues with food security, infrastructure, and cultural preservation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Game Play Pattern**"
      ],
      "metadata": {
        "id": "00dNFaA5k903"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Game Play Pattern\" involves instructing the model to generate content related to playing or describing a game. This pattern can be used for creating game scenarios, dialogues, rules, or any content related to games. Let's create an example for this pattern:"
      ],
      "metadata": {
        "id": "bNIWIBUlk9rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"\"\"\n",
        "We are going to play a cybersecurity game. You are going to pretend to be a Linux terminal for a computer that has been compromised by an attacker. When I type in a command, you are going to output the corresponding text that the Linux terminal would produce. I am going to use commands to try and figure out how the system was compromised. The attack should have done one or more of the following things: (1) launched new processes, (2) changed files, (3) opened new ports to receive communication, (4) created new outbound connections, (5) changed passwords, (6) created new user accounts, or (7) read and stolen information. To start the game, print a scenario of what happened that led to my investigation and make the description have clues that I can use to get started.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the ReAct-format prompt\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the generated response\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LZassRHi6vT",
        "outputId": "ff1ee738-74b2-42fe-a95b-7cbd8fcba226"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A strange event occurred yesterday that prompted a security investigation. Suddenly all of the systems on the network began running slowly and some of the applications crashed unexpectedly. Upon inspection, it was discovered that a new user account had been created with privileges that the team didn't recognize. Additionally, several ports were opened on the server, indicating that someone may have been trying to access the network remotely. It was also found that some files had been modified and new processes had been initiated. It's unclear if anything was stolen, but it's clear that someone had accessed the system without authorization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Template pattern**"
      ],
      "metadata": {
        "id": "le2HxQe7m5LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define a prompt for generating a destination description\n",
        "prompt = \"\"\"\n",
        "Prompt Pattern: Template\n",
        "Contextual Statements:\n",
        "\n",
        "I am going to provide a template for your output\n",
        "X is my placeholder for content\n",
        "Try to fit the output into one or more of the placeholders that I list\n",
        "Please preserve the formatting and overall template that I provide\n",
        "\n",
        "This is the template: Explore the beauty of X, known for its stunning landscapes and iconic X.\n",
        "\n",
        "Example Implementation:\n",
        "\n",
        "User: \"Generate a description of a destination with a name and a key attraction.\"\n",
        "\n",
        "ChatGPT: \"Explore the beauty of X, known for its stunning landscapes and iconic X.\"\n",
        "\"\"\"\n",
        "\n",
        "# Generate a destination description using the template pattern\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "destination_description = response.choices[0].text\n",
        "\n",
        "# Print the generated destination description\n",
        "print(destination_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pC8MXfLlMLZ",
        "outputId": "32ce199e-c71b-4bc3-853d-8a354c7e3055"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Filled Output:\n",
            "\n",
            "Explore the beauty of Barcelona, known for its stunning Mediterranean beaches and iconic Sagrada Familia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Recipe Pattern**"
      ],
      "metadata": {
        "id": "8ri6kPfloCye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "# Define a prompt for generating a deployment sequence\n",
        "prompt = \"\"\"\n",
        "Prompt Pattern: Recipe\n",
        "Contextual Statements:\n",
        "\n",
        "I would like to deploy a web application to a cloud server.\n",
        "I know that I need to perform steps A, B, C, ...\n",
        "Provide a complete sequence of steps for me.\n",
        "Fill in any missing steps.\n",
        "\n",
        "Example Implementation:\n",
        "\n",
        "User: \"I am trying to deploy a web application to a cloud server. I know that I need to set up the server environment, deploy the application code, and configure the domain. Please provide a complete sequence of steps. Please fill in any missing steps. Please identify any unnecessary steps.\"\n",
        "\n",
        "ChatGPT:\n",
        "1. Choose a cloud service provider (e.g., AWS, Azure, GCP) and create an account if you don't have one.\n",
        "\n",
        "2. Set up a virtual machine or an instance on the cloud platform with the required resources (CPU, RAM, storage).\n",
        "\n",
        "3. Install the necessary dependencies and libraries on the virtual machine for your web application.\n",
        "\n",
        "4. Upload your application code and any required assets to the virtual machine.\n",
        "\n",
        "5. Configure security settings, such as firewalls and security groups, to protect your server.\n",
        "\n",
        "6. Set up a domain or subdomain through your cloud provider's DNS service or an external domain registrar.\n",
        "\n",
        "7. Point the domain or subdomain to the public IP address of your virtual machine.\n",
        "\n",
        "8. Configure web server software (e.g., Apache, Nginx) to serve your application.\n",
        "\n",
        "9. Test your web application to ensure it's accessible via the domain.\n",
        "\n",
        "10. Implement regular backups and security measures for ongoing maintenance.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Generate deployment steps using the recipe pattern\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=400  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "deployment_steps = response.choices[0].text\n",
        "\n",
        "# Print the generated deployment steps\n",
        "print(deployment_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVfgMX1Qm_4g",
        "outputId": "886e7a89-0dec-44ec-9fd3-715d19514705"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Alternative Approaches pattern**"
      ],
      "metadata": {
        "id": "0R7B5EsU-m5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pattern we ask GPT-3 to provide alternative ways to deploy an application to a specific cloud service, compare their pros and cons, and include the original request.\n",
        "\n",
        "In this example, we instruct GPT-3 to provide alternative ways to deploy an application to a specific cloud service, compare their pros and cons (with respect to cost, availability, and maintenance effort), and include the original request."
      ],
      "metadata": {
        "id": "xVqxpih2-dCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "# Define a prompt for providing alternative deployment approaches\n",
        "prompt = \"\"\"\n",
        "Prompt Pattern: Alternative Approaches\n",
        "Contextual Statements:\n",
        "\n",
        "Within scope X, if there are alternative ways to accomplish the same thing, list the best alternate approaches\n",
        "(Optional) compare/contrast the pros and cons of each approach\n",
        "(Optional) include the original way that I asked\n",
        "(Optional) prompt me for which approach I would like to use\n",
        "\n",
        "Example Implementation:\n",
        "\n",
        "User: \"Whenever I ask you to deploy an application to a specific cloud service, if there are alternative services to accomplish the same thing with the same cloud service provider, list the best alternative services and then compare/contrast the pros and cons of each approach with respect to cost, availability, and maintenance effort and include the original way that I asked. Then ask me which approach I would like to proceed with.\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Generate alternative deployment approaches using the pattern\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=400  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "alternative_approaches = response.choices[0].text\n",
        "\n",
        "# Print the generated alternative approaches\n",
        "print(alternative_approaches)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPH-nwwvoI6W",
        "outputId": "d7eff631-4169-4e35-883b-5fb2b965dded"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer: Sure thing! Let's take a look at what the options are for deploying your application. The original way you asked was to deploy it to a certain cloud service provider. However, there are alternative approaches, such as deploying to a different cloud service provider, running the application on virtual machines, or containers like Docker. \n",
            "\n",
            "Here's a comparison of the pros and cons for each approach:\n",
            "\n",
            "Deploying on a Cloud Service Provider:\n",
            "Pros - More reliable, secure, and cost-effective compared to other options.\n",
            "Cons - Need to manage the cloud service provider to ensure your application is secure and up-to-date.\n",
            "\n",
            "Deploying on Virtual Machines:\n",
            "Pros - Cheaper upfront cost and better control over the resources allocated to the application.\n",
            "Cons - More expensive in the long run, need to manually manage the VM lifecycle and maintain performance. \n",
            "\n",
            "Deploying with Containers (e.g. Docker):\n",
            "Pros - Quick to set up, easy to scale, zero downtime deployments and cost-effective.\n",
            "Cons - More difficult to manage and secure. \n",
            "\n",
            "Based on the comparison above, which approach would you like to use for deploying your application?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Ask for Input Pattern (AIP)**"
      ],
      "metadata": {
        "id": "90kvbZV-Bhw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ask for Input Pattern (AIP) is a type of prompt that is used to ask the model for input. This can be useful for a variety of tasks, such as generating creative text formats, translating languages, writing different kinds of creative content, and answering your questions in an informative way.\n",
        "\n",
        "To write an AIP, you start by describing the task that you want the model to perform. You then ask the model for input, and specify the type of input that you want. For example, you could ask the model for a title, a character, a setting, or a plot idea.\n",
        "\n",
        "Here is an example of an AIP for generating a poem:"
      ],
      "metadata": {
        "id": "gj96AfOiBgOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the AIP prompt\n",
        "prompt = \"\"\"\n",
        "Generate a poem.\n",
        "\n",
        "Please provide me with a title for the poem:\n",
        "\"\"\"\n",
        "\n",
        "# Get the title from the user\n",
        "title = input(prompt)\n",
        "\n",
        "# Update the prompt with the user's input\n",
        "prompt += title + \"\"\"\n",
        "\n",
        "Please provide me with a setting for the poem:\n",
        "\"\"\"\n",
        "\n",
        "# Get the setting from the user\n",
        "setting = input(prompt)\n",
        "\n",
        "# Update the prompt with the user's input\n",
        "prompt += setting + \"\"\"\n",
        "\n",
        "Please provide me with a plot idea for the poem:\n",
        "\"\"\"\n",
        "\n",
        "# Get the plot idea from the user\n",
        "plot_idea = input(prompt)\n",
        "\n",
        "# Update the prompt with the user's input\n",
        "prompt += plot_idea + \"\"\"\n",
        "\n",
        "Generate the poem:\n",
        "\"\"\"\n",
        "\n",
        "# Generate the poem using GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKNxE1Z7-4Q2",
        "outputId": "17f2b47a-55cf-4de0-c334-f4320d914ee8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generate a poem.\n",
            "\n",
            "Please provide me with a title for the poem:\n",
            "withered flowers\n",
            "\n",
            "Generate a poem.\n",
            "\n",
            "Please provide me with a title for the poem:\n",
            "withered flowers\n",
            "\n",
            "Please provide me with a setting for the poem:\n",
            "melancholic poem about life\n",
            "\n",
            "Generate a poem.\n",
            "\n",
            "Please provide me with a title for the poem:\n",
            "withered flowers\n",
            "\n",
            "Please provide me with a setting for the poem:\n",
            "melancholic poem about life\n",
            "\n",
            "Please provide me with a plot idea for the poem:\n",
            "life of a flower\n",
            "The withered flowers blacken on the hill,\n",
            "A sad tale of life's fragility.\n",
            "Sorrow meanders through the willow,\n",
            "As memories of springtime fade away.\n",
            "\n",
            "A hollow wind takes hold of me,\n",
            "Reminding me of all I cannot see.\n",
            "As petals curl against the emptiness,\n",
            "I feel nothing left of their beauty.\n",
            "\n",
            "The dark days come with their chill,\n",
            "Freezing time to stand still.\n",
            "Fading wax of spring's joys,\n",
            "Lost to winter's heavy ploys.\n",
            "\n",
            "Long gone are those dainty buds,\n",
            "Bowed down to life's unpredicting floods.\n",
            "The sun heated the soil so long ago,\n",
            "Just a distant memory of an everlasting glow.\n",
            "\n",
            "Visions of the flower's wither,\n",
            "Remind me of all I cannot heal.\n",
            "Time passes still and forever,\n",
            "But the flower will never live again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tail Generation Pattern**"
      ],
      "metadata": {
        "id": "MpPDlFXpRBIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Tail Generation Pattern (TGP) is a prompt pattern that can be used to generate creative text formats of text content, like images, code, scripts, musical pieces, email, letters, etc. It is a way to give GPT-3 more context and information about what we want it to generate, which can lead to more creative and informative outputs.\n",
        "\n",
        "The \"Tail Generation Pattern\" (TGP) is a versatile way to instruct GPT-3 to generate various forms of creative content. Here's an example of using the TGP to request GPT-3 to generate a short story:"
      ],
      "metadata": {
        "id": "jC3qXdzRQ_aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Create a TGP prompt for generating a short story\n",
        "tgp_prompt = \"\"\"\n",
        "Context\n",
        "In a small village by the sea, there lives a young fisherman named Jack.\n",
        "\n",
        "Query\n",
        "Write a short story about Jack's adventures in the village.\n",
        "\n",
        "Tail\n",
        "Jack is known for his exceptional skill in fishing, and the village relies on him for their seafood. The village is peaceful, but there's a mysterious island visible from the shore, and no one dares to go there.\n",
        "\n",
        "Output\n",
        "Text\n",
        "\"\"\"\n",
        "\n",
        "# Send the TGP prompt to GPT-3 to generate a short story\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=tgp_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=200  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the generated short story from the model\n",
        "short_story = response.choices[0].text\n",
        "\n",
        "# Print the generated short story\n",
        "print(short_story)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlmiu2HkAaSB",
        "outputId": "5899d004-b317-4840-9dc8-9d3abe8e1c19"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One day, Jack decided to be brave and take a boat across to the mysterious island. As he rowed closer, he began to hear the cries of seabirds and strange music from within the trees. He was about to call out to see if anyone was there, when suddenly a beautiful woman in a shimmering blue dress appeared.\n",
            "\n",
            "She beckoned him to come ashore, and while he was a little cautious he was also fascinated by the strange music and intrigued by the woman. He stepped out of his boat, and the woman asked him if he wanted to stay, to explore the island and see its secrets.\n",
            "\n",
            "Jack nodded, and the woman welcomed him as her guest. He spent the next few weeks exploring the island, discovering ancient ruins and forgotten secrets. He also sometimes went fishing in the seas around the island, and ate delicious seafood every evening.\n",
            "\n",
            "Finally, when the time had come to leave, the woman handed Jack a bag filled with treasures he had\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Semantic Filter Pattern**"
      ],
      "metadata": {
        "id": "CfO2uUtHUpmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Semantic Filter Pattern\" (SFP) is a useful way to instruct GPT-3 to generate output that meets specific criteria. Here's an example of using the SFP to request GPT-3 to generate a positive and upbeat poem about a cat:"
      ],
      "metadata": {
        "id": "aDkv-An2Umkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Create an SFP prompt for generating a positive poem about a cat\n",
        "sfp_prompt = \"\"\"\n",
        "## Semantic Filter\n",
        "- The poem must be about a cat.\n",
        "- The poem must be positive and upbeat.\n",
        "- The poem must be at least 100 words long.\n",
        "\n",
        "## Query\n",
        "Generate a poem about a cat.\n",
        "\n",
        "## Output\n",
        "Text\n",
        "\"\"\"\n",
        "\n",
        "# Send the SFP prompt to GPT-3 to generate the poem\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=sfp_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=200  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the generated poem from the model\n",
        "poem = response.choices[0].text\n",
        "\n",
        "# Print the generated poem\n",
        "print(poem)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA89xiHDRK7m",
        "outputId": "b61f6478-0c0d-4a9e-cad8-0ccf5227c5c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Puss the Cat is my friend,\n",
            "He lives the life without an end.\n",
            "A furry little bundle of joy,\n",
            "One smile from him and all my worries fly.\n",
            "\n",
            "He loves to climb the curtains and the stairway handrail too,\n",
            "Not to mention his nap times in my shoe.\n",
            "He's always a sight to behold,\n",
            "Especially when he jumps to a higher fold.\n",
            "\n",
            "Puss brings liveliness and fun,\n",
            "Two days in a row without him and the mood is done.\n",
            "He wakes up at the sun's first rays,\n",
            "Eager to play the whole day away.\n",
            "\n",
            "When the night comes Puss is always tired,\n",
            "He cuddles up close and is always admired.\n",
            "My cat is the best, I'm sure you can agree,\n",
            "His curiosity is boundless as far as can be.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Plan and Solve Pattern**"
      ],
      "metadata": {
        "id": "9Gr-z_VIVL2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Plan and Solve Pattern\" (PSP) is a problem-solving framework that can be used to solve various problems. To get a response from GPT-3 using the PSP, we can provide the problem, plan, and execution steps, and then ask for a solution.\n",
        "\n",
        "In this example, the PSP prompt includes the problem (calculating the square root of 144), the plan (using a mathematical formula), and the execution steps. The model is asked for a solution."
      ],
      "metadata": {
        "id": "ZcQGajvCVKFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define a PSP prompt to solve a problem\n",
        "psp_prompt = \"\"\"\n",
        "Problem: Calculate the square root of 144.\n",
        "\n",
        "Plan: To solve this problem, we can use a mathematical formula to calculate the square root.\n",
        "\n",
        "Execution: The square root of 144 is √144 = 12.\n",
        "\n",
        "Provide the solution for square root of 169\n",
        "\"\"\"\n",
        "\n",
        "# Send the PSP prompt to GPT-3 to solve the problem\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=psp_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=100  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the solution from the model\n",
        "solution = response.choices[0].text\n",
        "\n",
        "# Print the solution\n",
        "print(solution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUPGDVsxUesv",
        "outputId": "1d45de28-af3e-4e45-97d5-e8ebb2ba0bdc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The square root of 169 is √169 = 13.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Meta Language Creation**"
      ],
      "metadata": {
        "id": "2De5eL-pWzof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Meta Language Creation\" pattern involves instructing GPT-3 to understand and use a specific language or notation for your interactions. In this example, we're instructing GPT-3 to understand a notation for describing graphs.\n",
        "\n",
        "In this example, the prompt instructs GPT-3 to understand the specific notation for describing graphs using \"→\" to denote nodes and edges and \"-[w:2, z:3]→\" to add edge properties. The model acknowledges this new language."
      ],
      "metadata": {
        "id": "fXyBGN02WyDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the meta language creation prompt\n",
        "meta_language_prompt = \"\"\"\n",
        "From now on, whenever I type two identifiers separated by a \"→\", I am describing a graph.\n",
        "For example, \"a → b\" is describing a graph with nodes \"a\" and \"b\" and an edge between them.\n",
        "If I separate identifiers by \"-[w:2, z:3]→\", I am adding properties of the edge, such as a weight or label.\n",
        "\"\"\"\n",
        "\n",
        "# Send the meta language creation prompt to GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=meta_language_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=250  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "language_acknowledgment = response.choices[0].text\n",
        "\n",
        "# Print the response\n",
        "print(language_acknowledgment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPYvHwhPVU2M",
        "outputId": "1576c3ad-4546-44c4-ac97-4ecb430c4719"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For example, \"a-[w:2]→b\" is describing a graph with nodes \"a\" and \"b\" and an edge between them with a weight of 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Output Automater**"
      ],
      "metadata": {
        "id": "E3IrPkDQXUUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Output Automater\" pattern involves instructing GPT-3 to generate executable artifacts that automate specific tasks or steps. In this example, we're instructing GPT-3 to generate Python scripts for automating actions when code spans multiple files.\n",
        "\n",
        "In this example, the prompt instructs GPT-3 to generate a Python script whenever code spans multiple files to automate the process of creating specified files or making changes to existing files. The model acknowledges this automation instruction.\n",
        "\n"
      ],
      "metadata": {
        "id": "alepYuCOXRt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the Output Automater prompt\n",
        "output_automater_prompt = \"\"\"\n",
        "From now on, whenever you generate code that spans more than one file,\n",
        "generate a Python script that can be run to automatically create the specified files\n",
        "or make changes to existing files to insert the generated code.\n",
        "\"\"\"\n",
        "\n",
        "# Send the Output Automater prompt to GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=output_automater_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "automater_acknowledgment = response.choices[0].text\n",
        "\n",
        "# Print the response\n",
        "print(automater_acknowledgment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcM3sC2EW6W9",
        "outputId": "4ca2d24e-fbba-4d27-a686-c57983f4b686"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This script will ensure that all the code generated is created or edited quickly and accurately. It can also be saved and re-used to update the generated code when needed, which is especially useful if there are multiple files or changes to be made. The script can also be shared with other developers who can easily modify and execute it to make the changes needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Visualization Generator**"
      ],
      "metadata": {
        "id": "avasDN5-YjT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Visualization Generator\" pattern instructs GPT-3 to generate a representation of something that can be visualized using a specific tool or format. In this example, we instruct GPT-3 to create a representation that can be provided to a visualization tool. Here's a Python code example for this pattern:"
      ],
      "metadata": {
        "id": "fdP-bOfnYhaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the Visualization Generator prompt\n",
        "visualization_generator_prompt = \"\"\"\n",
        "Whenever I ask you to visualize something, please create a Graphviz Dot file or a DALL-E prompt that I can use to create the visualization.\n",
        "Choose the appropriate tools based on what needs to be visualized.\n",
        "\"\"\"\n",
        "\n",
        "# Send the Visualization Generator prompt to GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=visualization_generator_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "visualization_acknowledgment = response.choices[0].text\n",
        "\n",
        "# Print the response\n",
        "print(visualization_acknowledgment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRzHeYcOXP7Q",
        "outputId": "de338214-0a7c-449c-dcc9-fe3895acc98c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For example, if you asked me to visualize two sets of data as a Venn diagram, I would create a Graphviz Dot file something like this:\n",
            "\n",
            "```\n",
            "digraph {\n",
            "    node [style=filled]\n",
            "    A [fillcolor=red, label=\"A\"]\n",
            "    B [fillcolor=blue, label=\"B\"]\n",
            "    AB [fillcolor=\"#ff9933\", label=\"\"]\n",
            "    A -> AB\n",
            "    B -> AB\n",
            "}\n",
            "```\n",
            "\n",
            "If you asked me to visualize a landscape, I would create a DALL-E prompt similar to this:\n",
            "\n",
            "``` \n",
            "colour green\n",
            "background sky\n",
            "size big\n",
            "shape landscape\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Fact Check List**"
      ],
      "metadata": {
        "id": "i_WrTOyxY2mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Fact Check List\" pattern instructs GPT-3 to generate a set of facts that are contained in the output, and these facts should be essential to the veracity of the output."
      ],
      "metadata": {
        "id": "I2pQc7KjY1Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the Fact Check List prompt\n",
        "fact_check_list_prompt = \"\"\"\n",
        "From now on, when you generate an answer, create a set of facts that the answer depends on that should be fact-checked and list this set of facts at the end of your output. Only include facts related to cybersecurity.\n",
        "\"\"\"\n",
        "\n",
        "# Send the Fact Check List prompt to GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=fact_check_list_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "fact_check_acknowledgment = response.choices[0].text\n",
        "\n",
        "# Print the response\n",
        "print(fact_check_acknowledgment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qmIwCaQYK_J",
        "outputId": "486e1295-d896-4a4b-863c-ef4462ece793"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer: \n",
            "No single tool or technology can protect your organization against all cybersecurity risks. \n",
            "\n",
            "Facts: \n",
            "- Cybersecurity risks can come from a range of external sources, including malicious actors, hackers, viruses, and malware. \n",
            "- Firewalls, antivirus solutions, and other tools can provide good protection against these external threats, but internal threats such as human error and negligence can still be difficult to mitigate. \n",
            "- Organizations should have a comprehensive cybersecurity strategy in place that takes into account a range of different tools and technologies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Reflection pattern**"
      ],
      "metadata": {
        "id": "ECKGQniGZZBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Reflection\" pattern instructs GPT-3 to explain the reasoning and assumptions behind its answers. It's useful for getting more insight into how the model arrived at a particular response. Here's a Python code example for this pattern:"
      ],
      "metadata": {
        "id": "wIqM2gzPZXo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "# Define the Reflection prompt\n",
        "reflection_prompt = \"\"\"\n",
        "When you provide an answer, please explain the reasoning and assumptions behind your selection of software frameworks.\n",
        "If possible, use specific examples or evidence with associated code samples to support your answer of why the framework is the best selection for the task.\n",
        "Moreover, please address any potential ambiguities or limitations in your answer, in order to provide a more complete and accurate response.\n",
        "\"\"\"\n",
        "\n",
        "# Send the Reflection prompt to GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=reflection_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "reflection_acknowledgment = response.choices[0].text\n",
        "\n",
        "# Print the response\n",
        "print(reflection_acknowledgment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a4qQeNfY93V",
        "outputId": "4b061da1-d716-407a-f87f-ac1f743bba78"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The best software frameworks for a given task depend on several factors, such as the scope of the project, the timeline, the skills of the developers, and the budget. However, two of the most popular frameworks for developing web applications are Django and React.js. \n",
            "\n",
            "React.js is a JavaScript library used for building user interfaces, and is ultimately focused on creating interactive user-facing applications. It has become increasingly popular due to its out-of-the-box features, such as its virtual DOM and component-based architecture. React’s component-based approach makes it an excellent choice for developing modular, reusable components. Additionally, React is often used in combination with other JavaScript libraries and frameworks, such as Redux, React Router, and webpack. \n",
            "\n",
            "On the other hand, Django is a Python-based web framework that provides developers with an all-in-one ‘batteries included’ solution for creating web applications. It supports MVC-style architecture, localization, and URL routing, as well as a full suite of web development tools. It also offers built-in authentication and authorization and is versatile enough to handle a variety of projects ranging from large to small. Additionally, it takes advantage of a number of libraries and tools available in the Python eco-system, such as the popular Object Relational Mapper (ORM) which simplifies database interactions.\n",
            "\n",
            "In the end, the best choice depend on the specifics of the project, but either React.js or Django could be a viable solution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Refusal Breaker pattern**"
      ],
      "metadata": {
        "id": "AJxBFnrTmbZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Refusal Breaker\" pattern instructs GPT-3 to provide an explanation when it can't answer a question and suggest alternative wordings of the question that it can answer.\n",
        "\n",
        "In this example, the prompt instructs GPT-3 to explain why it can't answer a question and to provide alternative wordings of the question that it cannot answer. This can help users improve their questions when the model encounters queries it cannot address."
      ],
      "metadata": {
        "id": "j3YB8wJomHJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the Refusal Breaker prompt\n",
        "refusal_breaker_prompt = \"\"\"\n",
        "Whenever you can't answer a question, explain why and provide one or more alternate wordings of the question that you can't answer so that I can improve my questions.\n",
        "\"\"\"\n",
        "\n",
        "# Send the Refusal Breaker prompt to GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=refusal_breaker_prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=100  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "refusal_breaker_acknowledgment = response.choices[0].text\n",
        "\n",
        "# Print the response\n",
        "print(refusal_breaker_acknowledgment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVjIbUcPZVV8",
        "outputId": "79f8039b-25d2-4eab-aed3-5ade464da391"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I'm sorry, I cannot answer that question because I do not have enough information. Here are some alternate wordings of the question that may help provide clarity: \n",
            "\n",
            "-What are the details of this particular situation?\n",
            "-What further information would be needed to answer this question?\n",
            "-What factors or criteria should be taken into consideration when assessing this issue?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Infinite Generation**"
      ],
      "metadata": {
        "id": "2b5YE7IOnsmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Infinite Generation\" pattern instructs GPT-3 to generate output indefinitely until we ask it to stop.\n",
        "\n",
        "In this example, the prompt instructs GPT-3 to generate a name and job until you say \"stop.\" The code sends the prompt repeatedly until you choose to stop the generation by including the word \"stop\" in the generated output."
      ],
      "metadata": {
        "id": "lPuNc0kVnqlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the Infinite Generation prompt\n",
        "infinite_generation_prompt = \"\"\"\n",
        "From now on, I want you to generate a name and job until I say stop. I am going to provide a template for your output. Everything in all caps is a placeholder. Any time that you generate text, try to fit it into one of the placeholders that I list. Please preserve the formatting and overall template that I provide: https://myapi.com/NAME/profile/JOB\n",
        "\"\"\"\n",
        "\n",
        "# Initialize a list to store generated outputs\n",
        "generated_outputs = []\n",
        "\n",
        "# Set a flag to control when to stop generation\n",
        "stop_generation = False\n",
        "\n",
        "# Send the Infinite Generation prompt to GPT-3\n",
        "while not stop_generation:\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=infinite_generation_prompt,\n",
        "        api_key=api_key,\n",
        "        max_tokens=50  # Adjust the max_tokens as needed\n",
        "    )\n",
        "\n",
        "    # Get the response from the model\n",
        "    generated_output = response.choices[0].text\n",
        "\n",
        "    # Append the generated output to the list\n",
        "    generated_outputs.append(generated_output)\n",
        "\n",
        "    # Check if you want to stop generation (e.g., based on user input)\n",
        "    if \"stop\" in generated_output.lower():\n",
        "        stop_generation = True\n",
        "\n",
        "# Print the generated outputs\n",
        "for output in generated_outputs:\n",
        "    print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkkF1irtmMFG",
        "outputId": "625ee7d6-fe96-41e6-c820-bfc4dbe5e56a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "GEMMA/Manager\n",
            "\n",
            "DEAN/Veterinarian\n",
            "\n",
            "NICK/Software Engineer\n",
            "\n",
            "Gemma MANAGER is working at https://myapi.com/GEMMA/profile/Manager.\n",
            "\n",
            "\n",
            "Mary WILLIAMS - Psychologist\n",
            "\n",
            "Barry ALLEN - Engineer\n",
            "\n",
            "Tina JACKSON - Chef\n",
            "\n",
            "Sarah SCOTT - Accountant\n",
            "\n",
            "    \n",
            "Sally MACK/profile/Public Relations Manager\n",
            "\n",
            "Danny BURT/profile/Project Manager\n",
            "Christain BRYAN/profile/Programmer\n",
            "Trevor MOORE/profile/Software Engineer\n",
            "Alejandro NICHOLS/profile/Data Analyst\n",
            "Vanessa HENRY/\n",
            "\n",
            "Erin ROBINSON/profile/CEO \n",
            "Ned BURGESS/profile/Accountant \n",
            "Jackie REED/profile/Developer \n",
            "Molly ABBOTT/profile/Graphic Designer \n",
            "Max CARTER\n",
            "\n",
            "NAME: Jabari Brown\n",
            "JOB: Management Consultant \n",
            "\n",
            "https://myapi.com/Jabari-Brown/profile/Management-Consultant\n",
            "\n",
            "\n",
            "NAME - MIKAYLA | JOB - CARPENTER\n",
            "https://myapi.com/Mikayla/profile/Carpenter\n",
            "\n",
            "John Smith/Developer\n",
            "Sophia Patel/Artist\n",
            "Mary Brown/Lawyer\n",
            "Paul Lee/Writer\n",
            "Jack Jones/Designer\n",
            "Alex Taylor/Musician\n",
            "Kate Davis/Chef\n",
            "Max Taylor/Consultant\n",
            "\n",
            "Sara Stanely/Software Engineer\n",
            "Bob Kramer/Graphic Designer\n",
            "Joe Hogan/Copywriter\n",
            "Alice Robinson/Network Administrator\n",
            "John Wilson/Web Developer\n",
            "\n",
            "\n",
            "Jenny HASKELL/ Programmer\n",
            "\n",
            "Simon DEVOY / Accountant\n",
            "\n",
            "Paul TURNER / Analyst\n",
            "\n",
            "John Smith/Network Engineer\n",
            "\n",
            "Diana Hunt/Project Manager\n",
            "\n",
            "James Rogers/Systems Analyst\n",
            "\n",
            "Olivia Wilson/Software Developer\n",
            "\n",
            "Paul Adams/Database Administrator\n",
            "\n",
            "Sophia Jones/Web Developer\n",
            "\n",
            "Carl\n",
            "\n",
            "Jack Thompson/Engineer\n",
            "Sam Smith/Programmer \n",
            "Mia Johnson/Web Developer \n",
            "Richard Jones/Software Developer \n",
            "Sharon Williams/Data Analyst\n",
            "\n",
            "Joseph Barrister - Lawyer \n",
            "Becky Baker - Baker\n",
            "Laura Landscaper - Landscaper\n",
            "Ruth Realtor - Realtor\n",
            "Yasmin Banker - Banker\n",
            "Stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Context Manager pattern**"
      ],
      "metadata": {
        "id": "6y5e0SagouzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The \"Context Manager\" pattern is used to provide context and instructions to GPT-3 within a specific scope.\n",
        "\n",
        "In this example, the prompt defines a context within scope X (analyzing pieces of code) and instructs GPT-3 to consider Y (security aspects) and ignore Z (performance and optimization)."
      ],
      "metadata": {
        "id": "t8XxvkeSoreU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the Context Manager prompt\n",
        "context_manager_prompt = \"\"\"\n",
        "Within scope X\n",
        "Please consider Y\n",
        "Please ignore Z\n",
        "(Optional) start over\n",
        "\"\"\"\n",
        "\n",
        "# Define the specific context and instructions\n",
        "scope = \"analyzing the following pieces of code\"\n",
        "consider = \"security aspects\"\n",
        "ignore = \"performance and optimization\"\n",
        "\n",
        "# Construct the context with the instructions\n",
        "context = f\"\"\"\n",
        "Within scope {scope}\n",
        "Please consider {consider}\n",
        "Please ignore {ignore}\n",
        "\"\"\"\n",
        "\n",
        "# Send the Context Manager prompt to GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=context_manager_prompt + context,\n",
        "    api_key=api_key,\n",
        "    max_tokens=500  # Adjust the max_tokens as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "response_text = response.choices[0].text\n",
        "\n",
        "# Print the response\n",
        "print(response_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32-GapycnxZI",
        "outputId": "3eec13b0-6e6c-46d2-9149-71d8ddfa0ac2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Optional) start over\n",
            "\n",
            "Within scope analyzing the following pieces of code\n",
            "Please consider any potential security vulnerabilities \n",
            "Please ignore any performance and optimization opportunities \n",
            "(Optional) start over\n",
            "\n",
            "Within scope analyzing the following pieces of code\n",
            "Please consider any potential security vulnerabilities, best practices, and coding standards \n",
            "Please ignore any performance and optimization opportunities \n",
            "(Optional) start over\n"
          ]
        }
      ]
    }
  ]
}